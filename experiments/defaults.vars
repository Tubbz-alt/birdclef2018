# default settings
# You'd usually leave these unchanged, and override specific settings
# via another file passed with --vars, or directly via --var KEY=VALUE.

# label filename
label_file=fg.tsv

# spectrogram extraction
sample_rate=22050
frame_len=1024
fps=70

# filterbank: mel, mel_learn or none
filterbank=mel

# mel filterbank settings (if used), and upper frequency for network input
mel_bands=80
mel_min=27.5
mel_max=8000

# magnitude scale: log, log1p, log1p_learn, pow_learn or none
magscale=log

# input block length (in seconds)
len_min=5
len_max=30
# number of buckets between min and max length for bucketed mini-batches
len_buckets=10

# metadata input settings
# input_meta expects a comma-separated list of data to use
# available data sources are: date,time,latitude,longitude,elevation
input_meta=
# date and time support a customizable encoding for the network
input_meta.date_encoding=circular
input_meta.date_encoding.size=12
input_meta.time_encoding=circular
input_meta.time_encoding.size=12
# other values support some rough standardization
input_meta.latitude_mean=-6.4
input_meta.latitude_std=12
input_meta.longitude_mean=-62
input_meta.longitude_std=12
input_meta.elevation_mean=967
input_meta.elevation_std=988
# for training, some Gaussian noise can be added
input_meta.date_noise=7
input_meta.time_noise=10
input_meta.latitude_noise=1
input_meta.longitude_noise=1
input_meta.elevation_noise=20
# metadata items can be dropped at random during training
input_meta.dropout=0
# missing or dropped metadata will be indicated by a binary attribute;
# but it still requires some value to be filled in: zero, random or mean
input_meta.missing=zero

# training settings
learn_scheme=adam
batchsize=16
initial_eta=0.001
eta_decay=0.85
eta_decay_every=1
eta_cycle=0
momentum=0.9
epochs=20
epochsize=2000
l2_decay=0
pred_clip=1e-7
cost=ce_bgtopk:10

# training settings for first `first_params` (for learnable spectrograms)
first_params=1
first_params_eta_scale=1
# track values of `first_params` every N batches in {MODELFILE%npz}hist.npz
first_params_log=0

# data augmentation settings
spect_augment=0
spline_order=2
max_stretch=0
max_shift=0
max_db=0
mixup=0
mixup2=0
bg_threads=6
bg_processes=0

# network architecture: ismir2016 or custom layer list
arch=ismir2016
# which nonlinearity to use
arch.nonlin=lrelu
# whether to use batch normalization (1) or not (0)
arch.batch_norm=1
# how many groups to use for conv layers (given as "conv" in the layer list)
arch.conv_groups=1
# whether to use dropout for conv layers: independent, channels, bands, none
arch.convdrop=none
# how many groups to use for dense layers (given as "Conv" in the layer list)
arch.dense_groups=1
# optional nonlinearity to apply before global pooling
arch.prepool=none
# how to pool over time
arch.pool=max
# whether to end with sigmoid, softmax or linear
arch.output=sigmoid
# apply random filterbank stretching (requires filterbank=mel_learn)
arch.mel_rndspread=0
# optionally disable mel filterbank training for filterbank=mel_learn
arch.mel_trainable=1

# network architecture for metadata (requires input_meta to be set)
arch_meta=
arch_meta.nonlin=lrelu
arch_meta.batch_norm=0
arch_meta.fulldrop=0
